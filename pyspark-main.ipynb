{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python 3.6\n",
    "- findspark==1.4.2\n",
    "- pyspark==3.0.0\n",
    "- scipy == 1.5.1\n",
    "- numpy == 1.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "from math import log2\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Files Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Reference:\n",
    "> https://github.com/jhh37/lrmf-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(savepath, loadpath):\n",
    "    matrix = loadmat(loadpath)\n",
    "    matrix = matrix['M']\n",
    "    matrix[np.isnan(matrix)] = 0\n",
    "    norm_factor = max(matrix.min(), matrix.max(), key=abs)\n",
    "    matrix /= norm_factor\n",
    "    matrix = matrix.T\n",
    "    double_matrix = np.concatenate((matrix,matrix),axis=0)\n",
    "    triple_mat = np.concatenate((matrix,matrix,matrix),axis=0)\n",
    "    np.savetxt(savepath + 'jester2_row.txt',matrix,fmt='%.2f')\n",
    "    np.savetxt(savepath + 'jester2_row_b1.txt',double_matrix,fmt='%.2f')\n",
    "    np.savetxt(savepath + 'jester2_row_b2.txt',triple_mat,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 4.8 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "savepath='./data-files/'\n",
    "loadpath='./data-files/Jester_2.mat'\n",
    "if not os.path.isdir(savepath):\n",
    "    os.mkdir(savepath)\n",
    "generate_data(savepath,loadpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Method](#1.-Naive-Method)\n",
    "\n",
    "[Sampling Method](#2.-Sampling-Method-(DIMSUM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Execution should be carried out in sequence because of cross-referencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > RDD - these are the elements that run and operate on multiple nodes to do parallel processing on a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(ar):\n",
    "    ar = np.array(list(ar))\n",
    "    return ar.reshape(-1,1)*ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_func(a,b,n):\n",
    "    a,b = np.array(a), np.array(b)\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ATA_naive(mat):\n",
    "    res = mat\\\n",
    "        .map(lambda x: map_func(x))\\\n",
    "        .fold(0,add)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_wo_numpy(ar,n):\n",
    "    ar = list(ar)\n",
    "    l = [[0 for i in range(n)] for j in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            l[i][j] = ar[i]*ar[j]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_wo_numpy(a,b):\n",
    "    a,b = list(a), list(b)\n",
    "    n = len(a)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            a[i][j] += b[i][j]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ATA_naive_wo_numpy(mat):\n",
    "    n = len(mat.first())\n",
    "    res = mat\\\n",
    "        .map(lambda x: map_wo_numpy(x,n))\\\n",
    "        .fold([[0 for i in range(n)]for j in range(n)],fold_wo_numpy)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"MatATA\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def splitter(l):\n",
    "    return [float(x) for x in l.strip().split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes ~ **3 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2.5 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "res = calc_ATA_naive(mat_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes **~30 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = calc_ATA_naive_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row_b1.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes **~ 50 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = calc_ATA_naive_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On an even larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row_b2.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes ~ **3m 4sec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = calc_ATA_naive_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using mapPartition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References for using mapPartitions\n",
    "> https://stackoverflow.com/questions/44222307/spark-rdd-default-number-of-partitions\n",
    "\n",
    "> https://github.com/mahmoudparsian/pyspark-tutorial/tree/master/tutorial/map-partitions\n",
    "\n",
    "> https://medium.com/parrot-prediction/partitioning-in-apache-spark-8134ad840b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I've implemented the `without numpy` version only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_part(ar,n):\n",
    "    l = [[0 for i in range(n)] for j in range(n)]\n",
    "    for a in ar:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                l[i][j] += a[i]*a[j]\n",
    "    \n",
    "    yield l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_map_part_naive_wo_numpy(mat):\n",
    "    n = len(mat.first())\n",
    "    zeroVal = [[0 for i in range(n)]for j in range(n)]\n",
    "    res = mat\\\n",
    "        .mapPartitions(lambda x: map_part(x,n))\\\n",
    "        .fold(zeroVal, fold_wo_numpy)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On my machine the total number of cores(physical+logical) = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row.txt', 12).map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 12 partitions: ~ **23 s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = calc_map_part_naive_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling Method (DIMSUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mapPartitions\n",
    "- fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caching RDDs** in Spark: It is one mechanism to speed up applications that access the same RDD multiple times. An RDD that is not cached, nor checkpointed, is re-evaluated again each time an action is invoked on that RDD. There are two function calls for caching an RDD: cache() and persist(level: StorageLevel). The difference among them is that cache() will cache the RDD into memory, whereas persist(level) can cache in memory, on disk, or off-heap memory according to the caching strategy specified by level.\n",
    "\n",
    "- A good reference: https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplier(ar):\n",
    "    ar = np.array(ar)\n",
    "    return ar*ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-computing the magnitudes of each column-vector\n",
    "def cols_magnitude(mat):\n",
    "    res = mat\\\n",
    "        .map(lambda x: multiplier(x))\\\n",
    "        .fold(0, add) # with numpy operator.add works fine\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplier_wo_numpy(ar):\n",
    "    return [a*a for a in ar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arr(a,b):\n",
    "    return [(ai+bi) for (ai,bi) in zip(a,b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-computing the magnitudes of each column-vector\n",
    "def cols_magnitude_wo_numpy(mat):\n",
    "    n = len(mat.first())\n",
    "    res = mat\\\n",
    "        .map(lambda x: multiplier_wo_numpy(x))\\\n",
    "        .fold([0 for i in range(n)], add_arr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "col_mags_numpy = cols_magnitude(mat_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "col_mags_wo_numpy = cols_magnitude_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimsum_map_part(ar,n,gamma,mag_cols):\n",
    "    l = [[0 for i in range(n)] for j in range(n)]\n",
    "    for a in ar:\n",
    "        for i in range(n):\n",
    "            if random() < (gamma/mag_cols[i]):\n",
    "                for j in range(i+1,n):\n",
    "                    temp = a[i]*a[j]\n",
    "                    l[i][j] += temp\n",
    "                    l[j][i] += temp\n",
    "    yield l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimsum_fold = fold_wo_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimsum_wo_numpy(mat, mag_cols, gamma, n_cols):\n",
    "    # gamma generally 2*log2(n)\n",
    "    if gamma<1:\n",
    "        return \"Error: Please provide gamma > 1\"\n",
    "    \n",
    "    n = n_cols\n",
    "    zeroVal = [[0 for i in range(n)]for j in range(n)]\n",
    "    res = mat\\\n",
    "        .mapPartitions(lambda x: dimsum_map_part(x,n,gamma,mag_cols))\\\n",
    "        .fold(zeroVal, dimsum_fold)\n",
    "    \n",
    "    # Now we restore what we normalized using gamma and col_mags  \n",
    "    for i in range(n):\n",
    "        for j in range(i,n):\n",
    "            if i==j:\n",
    "                res[i][j] = mag_cols[i]\n",
    "            elif (gamma / mag_cols[i]) < 1:\n",
    "                res[i][j] *= (mag_cols[i]/ gamma)\n",
    "            res[j][i] = res[i][j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(mat_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2*log2(n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes only ~ **2 sec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = dimsum_wo_numpy(mat_rdd, col_mags_wo_numpy, gamma, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On the larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row_b1.txt', 12).map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(mat_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2*log2(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "col_mags_wo_numpy = cols_magnitude_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = dimsum_wo_numpy(mat_rdd, col_mags_wo_numpy, gamma, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### On an even larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./data-files/jester2_row_b2.txt', 12).map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = len(mat_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2*log2(n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "col_mags_wo_numpy = cols_magnitude_wo_numpy(mat_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = dimsum_wo_numpy(mat_rdd, col_mags_wo_numpy, gamma, n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}