{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './test_data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"WCount\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .filter(lambda line: len(line)>1) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > RDD - these are the elements that run and operate on multiple nodes to do parallel processing on a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(filename).rdd.map(lambda r: r[0]) # RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. At ultrices mi tempus imperdiet nulla malesuada pellentesque. Vitae congue mauris rhoncus aenean vel elit. Arcu dui vivamus arcu felis bibendum ut tristique et egestas. Nunc non blandit massa enim nec dui nunc mattis. Amet facilisis magna etiam tempor orci eu lobortis elementum. Quis risus sed vulputate odio ut enim. Leo vel orci porta non pulvinar neque laoreet suspendisse. In tellus integer feugiat scelerisque. Diam sit amet nisl suscipit adipiscing bibendum est ultricies integer. Lacinia quis vel eros donec ac odio tempor. Sit amet consectetur adipiscing elit ut aliquam purus sit. Aliquet eget sit amet tellus cras. Eget est lorem ipsum dolor sit amet consectetur adipiscing elit. Ut ornare lectus sit amet est.',\n",
       " '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 165 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduced_data = lines.flatMap(lambda x: x.split(' ')) \\ # flatmap returns RDD\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "     .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 751 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output = reduced_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lorem', 570),\n",
       " ('ipsum', 3420),\n",
       " ('consectetur', 3990),\n",
       " ('sed', 12540),\n",
       " ('do', 570),\n",
       " ('labore', 570),\n",
       " ('magna', 3420),\n",
       " ('At', 1710),\n",
       " ('mi', 2280),\n",
       " ('nulla', 2850),\n",
       " ('malesuada', 4560),\n",
       " ('congue', 570),\n",
       " ('rhoncus', 3420),\n",
       " ('aenean', 2280),\n",
       " ('elit.', 2280),\n",
       " ('Arcu', 1710),\n",
       " ('felis', 3420),\n",
       " ('bibendum', 4560),\n",
       " ('Nunc', 2850),\n",
       " ('nunc', 4560),\n",
       " ('etiam', 1140),\n",
       " ('lobortis', 1710),\n",
       " ('elementum.', 1710),\n",
       " ('vulputate', 3420),\n",
       " ('odio', 6270),\n",
       " ('enim.', 1140),\n",
       " ('Leo', 1140),\n",
       " ('tellus', 5130),\n",
       " ('integer', 1710),\n",
       " ('feugiat', 3420),\n",
       " ('scelerisque.', 1140),\n",
       " ('Diam', 570),\n",
       " ('amet', 9690),\n",
       " ('suscipit', 1140),\n",
       " ('est', 3420),\n",
       " ('quis', 8550),\n",
       " ('Sit', 3990),\n",
       " ('purus', 5700),\n",
       " ('sit.', 3420),\n",
       " ('Aliquet', 2280),\n",
       " ('eget', 6840),\n",
       " ('ornare', 3420),\n",
       " ('lectus', 6840),\n",
       " ('', 5759),\n",
       " ('Non', 1710),\n",
       " ('interdum', 2850),\n",
       " ('Dui', 1140),\n",
       " ('Purus', 1140),\n",
       " ('faucibus', 5130),\n",
       " ('nisi', 2850),\n",
       " ('lacus', 2850),\n",
       " ('sapien', 2280),\n",
       " ('pellentesque', 8550),\n",
       " ('habitant', 570),\n",
       " ('et.', 1710),\n",
       " ('hac', 1710),\n",
       " ('habitasse', 1710),\n",
       " ('cursus.', 570),\n",
       " ('in', 6270),\n",
       " ('dictum', 2850),\n",
       " ('amet.', 3420),\n",
       " ('ipsum.', 1140),\n",
       " ('at', 7980),\n",
       " ('Mi', 570),\n",
       " ('commodo', 3990),\n",
       " ('Sed', 1140),\n",
       " ('lacinia', 570),\n",
       " ('at.', 570),\n",
       " ('Vivamus', 570),\n",
       " ('nibh', 5700),\n",
       " ('condimentum', 5130),\n",
       " ('venenatis', 2850),\n",
       " ('cras', 3420),\n",
       " ('neque.', 1140),\n",
       " ('Pulvinar', 570),\n",
       " ('ullamcorper', 3420),\n",
       " ('consequat.', 570),\n",
       " ('fringilla', 2850),\n",
       " ('augue', 3420),\n",
       " ('Porttitor', 570),\n",
       " ('Tellus', 1140),\n",
       " ('urna', 6270),\n",
       " ('mattis', 3990),\n",
       " ('Donec', 1710),\n",
       " ('dapibus', 1140),\n",
       " ('Orci', 1140),\n",
       " ('mauris.', 1140),\n",
       " ('scelerisque', 3420),\n",
       " ('fermentum', 2280),\n",
       " ('leo', 1710),\n",
       " ('Elit', 570),\n",
       " ('venenatis.', 1140),\n",
       " ('cursus', 2280),\n",
       " ('Consequat', 1140),\n",
       " ('diam', 7410),\n",
       " ('volutpat.', 1710),\n",
       " ('Cursus', 1140),\n",
       " ('molestie', 1140),\n",
       " ('iaculis', 1710),\n",
       " ('erat', 1140),\n",
       " ('adipiscing.', 570),\n",
       " ('Vulputate', 570),\n",
       " ('porttitor', 1140),\n",
       " ('Turpis', 570),\n",
       " ('Libero', 570),\n",
       " ('justo', 1710),\n",
       " ('phasellus', 2850),\n",
       " ('lorem.', 1140),\n",
       " ('vitae', 6270),\n",
       " ('Senectus', 570),\n",
       " ('gravida.', 1140),\n",
       " ('quisque.', 570),\n",
       " ('Habitasse', 570),\n",
       " ('quisque', 1710),\n",
       " ('sagittis', 2280),\n",
       " ('quam', 3990),\n",
       " ('ante', 570),\n",
       " ('nibh.', 1140),\n",
       " ('Aenean', 570),\n",
       " ('donec.', 570),\n",
       " ('Maecenas', 570),\n",
       " ('Sollicitudin', 1140),\n",
       " ('rutrum', 570),\n",
       " ('nunc.', 1140),\n",
       " ('massa.', 2280),\n",
       " ('Tristique', 570),\n",
       " ('pretium.', 570),\n",
       " ('congue.', 570),\n",
       " ('quis.', 1710),\n",
       " ('Iaculis', 570),\n",
       " ('lacus.', 570),\n",
       " ('purus.', 1710),\n",
       " ('convallis', 570),\n",
       " ('fringilla.', 570),\n",
       " ('bibendum.', 1140),\n",
       " ('Molestie', 570),\n",
       " ('commodo.', 570),\n",
       " ('varius', 2850),\n",
       " ('Quam', 1140),\n",
       " ('ante.', 570),\n",
       " ('tincidunt.', 570),\n",
       " ('Dis', 570),\n",
       " ('nascetur', 570),\n",
       " ('mus.', 570),\n",
       " ('Ullamcorper', 570),\n",
       " ('Commodo', 570),\n",
       " ('Tempus', 570),\n",
       " ('Pellentesque', 1140),\n",
       " ('facilisi', 570),\n",
       " ('Felis', 570),\n",
       " ('sociis', 570),\n",
       " ('natoque', 570),\n",
       " ('Convallis', 570),\n",
       " ('duis', 1710),\n",
       " ('augue.', 570),\n",
       " ('mollis', 570),\n",
       " ('potenti', 570),\n",
       " ('feugiat.', 570),\n",
       " ('Nisi', 570),\n",
       " ('Elementum', 570),\n",
       " ('metus', 1140),\n",
       " ('Ridiculus', 570),\n",
       " ('mus', 570),\n",
       " ('felis.', 570),\n",
       " ('Nibh', 570),\n",
       " ('Dolor', 570),\n",
       " ('pharetra.', 570),\n",
       " ('dolor', 2850),\n",
       " ('sit', 9690),\n",
       " ('amet,', 570),\n",
       " ('adipiscing', 5700),\n",
       " ('elit,', 570),\n",
       " ('eiusmod', 570),\n",
       " ('tempor', 3990),\n",
       " ('incididunt', 570),\n",
       " ('ut', 10260),\n",
       " ('et', 8550),\n",
       " ('dolore', 570),\n",
       " ('aliqua.', 570),\n",
       " ('ultrices', 2850),\n",
       " ('tempus', 1710),\n",
       " ('imperdiet', 2280),\n",
       " ('pellentesque.', 1710),\n",
       " ('Vitae', 1710),\n",
       " ('mauris', 3990),\n",
       " ('vel', 3990),\n",
       " ('dui', 2280),\n",
       " ('vivamus', 1140),\n",
       " ('arcu', 6270),\n",
       " ('tristique', 3990),\n",
       " ('egestas.', 1140),\n",
       " ('non', 4560),\n",
       " ('blandit', 1140),\n",
       " ('massa', 3990),\n",
       " ('enim', 6840),\n",
       " ('nec', 3990),\n",
       " ('mattis.', 1710),\n",
       " ('Amet', 2280),\n",
       " ('facilisis', 2280),\n",
       " ('orci', 5130),\n",
       " ('eu', 5700),\n",
       " ('Quis', 570),\n",
       " ('risus', 3420),\n",
       " ('porta', 2280),\n",
       " ('pulvinar', 3990),\n",
       " ('neque', 3990),\n",
       " ('laoreet', 2850),\n",
       " ('suspendisse.', 1140),\n",
       " ('In', 3420),\n",
       " ('nisl', 5130),\n",
       " ('ultricies', 3420),\n",
       " ('integer.', 1140),\n",
       " ('Lacinia', 570),\n",
       " ('eros', 570),\n",
       " ('donec', 3990),\n",
       " ('ac', 6270),\n",
       " ('tempor.', 570),\n",
       " ('elit', 1710),\n",
       " ('aliquam', 6270),\n",
       " ('cras.', 570),\n",
       " ('Eget', 1710),\n",
       " ('lorem', 3420),\n",
       " ('Ut', 1710),\n",
       " ('est.', 2280),\n",
       " ('suspendisse', 3420),\n",
       " ('libero.', 1140),\n",
       " ('viverra', 4560),\n",
       " ('tellus.', 570),\n",
       " ('morbi', 5130),\n",
       " ('senectus', 570),\n",
       " ('netus', 1140),\n",
       " ('platea', 2280),\n",
       " ('dictumst', 2850),\n",
       " ('vestibulum', 3420),\n",
       " ('gravida', 2850),\n",
       " ('turpis', 2280),\n",
       " ('Mattis', 1140),\n",
       " ('nisl.', 1140),\n",
       " ('egestas', 7410),\n",
       " ('Lectus', 1140),\n",
       " ('varius.', 1140),\n",
       " ('imperdiet.', 570),\n",
       " ('euismod', 2280),\n",
       " ('proin', 6840),\n",
       " ('id', 6840),\n",
       " ('a.', 1710),\n",
       " ('A', 1140),\n",
       " ('semper', 2850),\n",
       " ('auctor', 2850),\n",
       " ('ligula', 2280),\n",
       " ('libero', 2850),\n",
       " ('sem', 1710),\n",
       " ('tincidunt', 2850),\n",
       " ('velit.', 1140),\n",
       " ('arcu.', 570),\n",
       " ('Tincidunt', 1140),\n",
       " ('velit', 1140),\n",
       " ('in.', 1710),\n",
       " ('tortor.', 570),\n",
       " ('ultrices.', 570),\n",
       " ('Eu', 1140),\n",
       " ('vel.', 1140),\n",
       " ('luctus', 1140),\n",
       " ('a', 1710),\n",
       " ('Justo', 1140),\n",
       " ('aliquet', 2850),\n",
       " ('luctus.', 570),\n",
       " ('Ornare', 1140),\n",
       " ('elementum', 3420),\n",
       " ('eleifend.', 1140),\n",
       " ('Fermentum', 570),\n",
       " ('porta.', 570),\n",
       " ('pretium', 1140),\n",
       " ('pharetra', 5130),\n",
       " ('placerat.', 570),\n",
       " ('nulla.', 570),\n",
       " ('Facilisi', 570),\n",
       " ('urna.', 2850),\n",
       " ('Nisl', 570),\n",
       " ('auctor.', 570),\n",
       " ('fames', 570),\n",
       " ('Nulla', 570),\n",
       " ('Morbi', 570),\n",
       " ('Tortor', 1710),\n",
       " ('Pretium', 570),\n",
       " ('dignissim', 570),\n",
       " ('sem.', 570),\n",
       " ('consequat', 1140),\n",
       " ('nam', 2280),\n",
       " ('Et', 1710),\n",
       " ('Sapien', 570),\n",
       " ('proin.', 570),\n",
       " ('Neque', 570),\n",
       " ('aliquam.', 570),\n",
       " ('Nullam', 570),\n",
       " ('nisi.', 1140),\n",
       " ('Id', 570),\n",
       " ('blandit.', 570),\n",
       " ('Mus', 570),\n",
       " ('leo.', 570),\n",
       " ('Enim', 570),\n",
       " ('Faucibus', 570),\n",
       " ('posuere', 1710),\n",
       " ('dolor.', 570),\n",
       " ('Massa', 1140),\n",
       " ('volutpat', 2280),\n",
       " ('hendrerit.', 1140),\n",
       " ('Vel', 570),\n",
       " ('faucibus.', 1710),\n",
       " ('Integer', 570),\n",
       " ('Volutpat', 570),\n",
       " ('Sem', 570),\n",
       " ('tortor', 2850),\n",
       " ('sed.', 1710),\n",
       " ('Platea', 570),\n",
       " ('Risus', 1140),\n",
       " ('quam.', 1710),\n",
       " ('curabitur', 570),\n",
       " ('parturient', 570),\n",
       " ('montes', 570),\n",
       " ('ridiculus', 570),\n",
       " ('risus.', 570),\n",
       " ('Ultrices', 570),\n",
       " ('Tempor', 570),\n",
       " ('molestie.', 570),\n",
       " ('odio.', 570),\n",
       " ('Venenatis', 1140),\n",
       " ('aliquet.', 1140),\n",
       " ('Malesuada', 570),\n",
       " ('cum', 570),\n",
       " ('penatibus', 570),\n",
       " ('tempus.', 570),\n",
       " ('Urna', 570),\n",
       " ('aenean.', 570),\n",
       " ('Augue', 1140),\n",
       " ('Imperdiet', 570),\n",
       " ('porttitor.', 570),\n",
       " ('Suspendisse', 1140),\n",
       " ('nullam', 570),\n",
       " ('Feugiat', 570),\n",
       " ('praesent', 570),\n",
       " ('Eleifend', 570),\n",
       " ('Semper', 570),\n",
       " ('accumsan', 570),\n",
       " ('Ante', 570),\n",
       " ('Mauris', 570),\n",
       " ('habitant.', 570),\n",
       " ('id.', 2280),\n",
       " ('Consectetur', 570),\n",
       " ('Auctor', 570)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Method âœ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"GroupByMat\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(l):\n",
    "    i,j,v = l.strip().split(' ')\n",
    "    return tuple([int(i),(int(j),float(v))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working dope\n",
    "# mat_rdd = sc.textFile('./small_numbers.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working dope\n",
    "mat_rdd = sc.textFile('./test_data_jester2.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_rdd = sc.textFile('./test_jester2_big2.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 0.0)), (0, (1, 0.8109999999999999))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best working\n",
    "# join_results = mat_rdd\\\n",
    "#     .map(lambda l: (l[0],(l[1],l[2])))\\\n",
    "#     .join(mat_rdd2.map(lambda l: (l[0],(l[1],l[2]))))\\\n",
    "#     .map(lambda t: ((t[1][0][0],t[1][1][0]), t[1][0][1]*t[1][1][1]))\\\n",
    "#     .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best working\n",
    "join_results = mat_rdd\\\n",
    "    .join(mat_rdd2.map(lambda l: l))\\\n",
    "    .map(lambda t: ((t[1][0][0],t[1][1][0]), t[1][0][1]*t[1][1][1]))\\\n",
    "    .reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "collected_results = join_results.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_results = sorted(collected_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 0), 4508.473709000027)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((0, 0), 4508.473709000027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = './res_test_jester_data.txt'\n",
    "with open(fileName, 'w') as f:\n",
    "    for t in collected_results:\n",
    "        (i,j),v = t\n",
    "        f.write(f\"{i} {j} {v}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- (23500,100) --> (100,100)\n",
    "- rows in txt files 2350000\n",
    "- test_data_jester2.txt: 40MB\n",
    "- Time to collect results: 1m 30s\n",
    "\n",
    "---------------------------------------------\n",
    "- (47000,100) --> (100,100)\n",
    "- rows in txt file 4700000\n",
    "- test_jester2_big2.txt --> 80MB\n",
    "- Time to collect results: 9m 35s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redfn(l):\n",
    "    x = np.array([x[1][1] for x in l[1]])\n",
    "    return (x.sum()**2 - x.dot(x)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.13383441772821036), (0, 1, 0.7468473923116297)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (0, (0, 0.13383441772821036))\n",
    "y = (0, (0, 0.13383441772821036))\n",
    "\n",
    "v -> x[1][1] * y[1][1]\n",
    "k -> (x[1][0], y[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customRed(a,b):\n",
    "#     if a[0]==b[0]:\n",
    "    return ((a[1][0],b[1][0]),a[1][1]*b[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ind,val], [ind,val], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd.reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,0,1\n",
    "# 0,1,2\n",
    "# 0,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, 0.13383441772821036, 1, 0.7468473923116297, 2, 0.887473518814142)),\n",
       " (2, (0, 0.6743332231395642, 1, 0.5619214520690802, 2, 0.5996560847181328)),\n",
       " (1, (0, 0.528287706368358, 1, 0.6165535215356, 2, 0.5567147819096118))]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_rdd.map(lambda l: (l[0],tuple(l[1:]))).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [(0, 0.13383441772821036), (1, 0.7468473923116297), (2, 0.887473518814142)]),\n",
       " (2,\n",
       "  [(0, 0.6743332231395642), (1, 0.5619214520690802), (2, 0.5996560847181328)]),\n",
       " (1, [(0, 0.528287706368358), (1, 0.6165535215356), (2, 0.5567147819096118)])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mat_rdd.map(lambda l: (l[0],(l[1][0],l[1][1]))).reduceByKey(lambda x,y: y[1]).collect()\n",
    "# mat_rdd.groupByKey().mapValues(list).collect()\n",
    "# mat_rdd.groupByKey().reduceByKey(lambda x: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x1e4de5667b8>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x1e4de5664a8>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x1e4de566898>)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mat_rdd.groupBy(lambda x: x[0]).reduceByKey(lambda l: redfn(l)).collect() # mapValues(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = mat_rdd.groupBy(lambda x: x[0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 968 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.13383441772821036),\n",
       " (0, 1, 0.7468473923116297),\n",
       " (0, 2, 0.887473518814142)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mat_rdd.filter(lambda x: x[0]==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0, 0.528287706368358),\n",
       " (1, 1, 0.6165535215356),\n",
       " (1, 2, 0.5567147819096118)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 0, 0.528287706368358), (1, 0, 0.528287706368358)),\n",
       " ((1, 0, 0.528287706368358), (1, 1, 0.6165535215356)),\n",
       " ((1, 1, 0.6165535215356), (1, 0, 0.528287706368358)),\n",
       " ((1, 1, 0.6165535215356), (1, 1, 0.6165535215356)),\n",
       " ((1, 0, 0.528287706368358), (1, 2, 0.5567147819096118)),\n",
       " ((1, 1, 0.6165535215356), (1, 2, 0.5567147819096118)),\n",
       " ((1, 2, 0.5567147819096118), (1, 0, 0.528287706368358)),\n",
       " ((1, 2, 0.5567147819096118), (1, 1, 0.6165535215356)),\n",
       " ((1, 2, 0.5567147819096118), (1, 2, 0.5567147819096118))]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xv.cartesian(xv).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = mat_rdd.filter(lambda x: x[0]==1)\n",
    "tres = xv.cartesian(xv)\\\n",
    "        .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "        .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((1, 1), 0.3801382449179495),\n",
       " ((0, 2), 0.2941055752363894),\n",
       " ((2, 0), 0.2941055752363894),\n",
       " ((0, 1), 0.3257176457453761),\n",
       " ((1, 2), 0.34324445927729463),\n",
       " ((0, 0), 0.27908790069994044),\n",
       " ((2, 2), 0.3099313483966666),\n",
       " ((1, 0), 0.3257176457453761),\n",
       " ((2, 1), 0.34324445927729463)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tres.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 61.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(num_rows):\n",
    "    g = mat_rdd.filter(lambda x: x[0]==i)\n",
    "    res_arr.append(g.cartesian(g)\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[((0, 2), 0.11877450163969665),\n",
       "  ((2, 0), 0.11877450163969665),\n",
       "  ((1, 1), 0.5577810274026813),\n",
       "  ((0, 1), 0.09995388588185924),\n",
       "  ((1, 2), 0.662807283271968),\n",
       "  ((0, 0), 0.01791165136864911),\n",
       "  ((2, 2), 0.7876092465963553),\n",
       "  ((1, 0), 0.09995388588185924),\n",
       "  ((2, 1), 0.662807283271968)],\n",
       " [((1, 1), 0.3801382449179495),\n",
       "  ((0, 2), 0.2941055752363894),\n",
       "  ((2, 0), 0.2941055752363894),\n",
       "  ((0, 1), 0.3257176457453761),\n",
       "  ((1, 2), 0.34324445927729463),\n",
       "  ((0, 0), 0.27908790069994044),\n",
       "  ((2, 2), 0.3099313483966666),\n",
       "  ((1, 0), 0.3257176457453761),\n",
       "  ((2, 1), 0.34324445927729463)],\n",
       " [((0, 2), 0.4043680203832301),\n",
       "  ((2, 0), 0.4043680203832301),\n",
       "  ((1, 1), 0.3157557182954236),\n",
       "  ((0, 1), 0.378922303925007),\n",
       "  ((1, 2), 0.33695961786687256),\n",
       "  ((0, 0), 0.4547252958297933),\n",
       "  ((2, 2), 0.35958741993948046),\n",
       "  ((1, 0), 0.378922303925007),\n",
       "  ((2, 1), 0.33695961786687256)],\n",
       " [((0, 2), 0.11877450163969665),\n",
       "  ((2, 0), 0.11877450163969665),\n",
       "  ((1, 1), 0.5577810274026813),\n",
       "  ((0, 1), 0.09995388588185924),\n",
       "  ((1, 2), 0.662807283271968),\n",
       "  ((0, 0), 0.01791165136864911),\n",
       "  ((2, 2), 0.7876092465963553),\n",
       "  ((1, 0), 0.09995388588185924),\n",
       "  ((2, 1), 0.662807283271968)],\n",
       " [((1, 1), 0.3801382449179495),\n",
       "  ((0, 2), 0.2941055752363894),\n",
       "  ((2, 0), 0.2941055752363894),\n",
       "  ((0, 1), 0.3257176457453761),\n",
       "  ((1, 2), 0.34324445927729463),\n",
       "  ((0, 0), 0.27908790069994044),\n",
       "  ((2, 2), 0.3099313483966666),\n",
       "  ((1, 0), 0.3257176457453761),\n",
       "  ((2, 1), 0.34324445927729463)],\n",
       " [((0, 2), 0.4043680203832301),\n",
       "  ((2, 0), 0.4043680203832301),\n",
       "  ((1, 1), 0.3157557182954236),\n",
       "  ((0, 1), 0.378922303925007),\n",
       "  ((1, 2), 0.33695961786687256),\n",
       "  ((0, 0), 0.4547252958297933),\n",
       "  ((2, 2), 0.35958741993948046),\n",
       "  ((1, 0), 0.378922303925007),\n",
       "  ((2, 1), 0.33695961786687256)],\n",
       " [((0, 2), 0.11877450163969665),\n",
       "  ((2, 0), 0.11877450163969665),\n",
       "  ((1, 1), 0.5577810274026813),\n",
       "  ((0, 1), 0.09995388588185924),\n",
       "  ((1, 2), 0.662807283271968),\n",
       "  ((0, 0), 0.01791165136864911),\n",
       "  ((2, 2), 0.7876092465963553),\n",
       "  ((1, 0), 0.09995388588185924),\n",
       "  ((2, 1), 0.662807283271968)],\n",
       " [((0, 2), 0.11877450163969665),\n",
       "  ((2, 0), 0.11877450163969665),\n",
       "  ((1, 1), 0.5577810274026813),\n",
       "  ((0, 1), 0.09995388588185924),\n",
       "  ((1, 2), 0.662807283271968),\n",
       "  ((0, 0), 0.01791165136864911),\n",
       "  ((2, 2), 0.7876092465963553),\n",
       "  ((1, 0), 0.09995388588185924),\n",
       "  ((2, 1), 0.662807283271968)],\n",
       " [((1, 1), 0.3801382449179495),\n",
       "  ((0, 2), 0.2941055752363894),\n",
       "  ((2, 0), 0.2941055752363894),\n",
       "  ((0, 1), 0.3257176457453761),\n",
       "  ((1, 2), 0.34324445927729463),\n",
       "  ((0, 0), 0.27908790069994044),\n",
       "  ((2, 2), 0.3099313483966666),\n",
       "  ((1, 0), 0.3257176457453761),\n",
       "  ((2, 1), 0.34324445927729463)],\n",
       " [((0, 2), 0.4043680203832301),\n",
       "  ((2, 0), 0.4043680203832301),\n",
       "  ((1, 1), 0.3157557182954236),\n",
       "  ((0, 1), 0.378922303925007),\n",
       "  ((1, 2), 0.33695961786687256),\n",
       "  ((0, 0), 0.4547252958297933),\n",
       "  ((2, 2), 0.35958741993948046),\n",
       "  ((1, 0), 0.378922303925007),\n",
       "  ((2, 1), 0.33695961786687256)]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "[x.collect() for x in res_arr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"MultMat\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(l):\n",
    "    i,j,v = l.strip().split(' ')\n",
    "    return tuple([int(i),int(j),float(v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0.6556)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter('0 0 0.6556')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_rdd = sc.textFile('./sample_numbers.txt').map(lambda l: splitter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0.7608220245799869), (0, 1, 0.6460457520796447)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_res = mat_rdd.cartesian(mat_rdd).filter(lambda x: x[0][0] == x[1][0])\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_results = mat_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# mat_res.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_res = mat_res.map(lambda x: ' '.join([str(p) for p in [x[0][0], x[0][1], x[1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat_res.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a (3000,10) matrix\n",
    "time collect: 1m 32s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a (5000,25) matrix\n",
    "collect time: Erro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 2), 750.3056621030034), ((0, 6), 748.7303530580234)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_res.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 1 times, most recent failure: Lost task 1.0 in stage 1.0 (TID 2, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:323)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:350)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:803)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:982)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:391)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:488)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mat_res.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a (2000,20) matrix\n",
    "Time taken: to write to disk -> 2m 43s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 43s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# mat_res.coalesce(1,True).saveAsTextFile(\"file:///D:/Sem 6/Data Science/Post Midsem/project/test-result/result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a (100,10) matrix\n",
    "Time taken to write to disk -> 18.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# mat_res.saveAsTextFile(\"file:///D:/Sem 6/Data Science/Post Midsem/project/test-result/result.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a (3000, 30) matrix\n",
    "Time take: to write to disk -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"MultMat\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = lambda x: list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_rdd = spark.read.text('./jester2.txt').rdd.map(lambda l: map(float, l.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = spark.read.text('./test_data_jester2.txt').rdd.map(lambda r: r[0])\n",
    "# rows = sc.textFile('./test_data_jester2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_rdd = rows.map(lambda l: map(float, l.strip().split(' ')))\n",
    "rows_rdd = rows.map(lambda l: tuple(float(x) for x in l.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0), (0.0, 1.0, 0.8109999999999999)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0), (0.0, 1.0, 0.8109999999999999), (0.0, 2.0, 0.0)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = '0 0 1\\n0 1 3\\n1 0 4\\n 1 1 7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_rdd = sc.textFile('./test_data_jester2.txt').map(lambda l: tuple(float(x) for x in l.strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0), (0.0, 1.0, 0.8109999999999999)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_res = temp_rdd\\\n",
    "    .cartesian(temp_rdd).filter(lambda x: x[0][0] == x[1][0])\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 81.0 failed 1 times, most recent failure: Lost task 3.0 in stage 81.0 (TID 2150, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 81.0 failed 1 times, most recent failure: Lost task 3.0 in stage 81.0 (TID 2150, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeUTF(PythonRDD.scala:465)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:285)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp_ress = temp_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rdd_temp = sc.parallelize(temp_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0), (0.0, 1.0, 0.8109999999999999)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_temp.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_temp_res = rdd_temp\\\n",
    "    .cartesian(rdd_temp).filter(lambda x: x[0][0] == x[1][0])\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_tress = rdd_temp_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_rdd = sc.parallelize([(0,0,1), (0,1, 3), (1,0, 4), (1,1,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ex_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vv = (((0, 0), 1), ((0, 0), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vv[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 0, 1), (0, 0, 1)),\n",
       " ((0, 0, 1), (0, 1, 3)),\n",
       " ((0, 1, 3), (0, 0, 1)),\n",
       " ((0, 1, 3), (0, 1, 3)),\n",
       " ((1, 0, 4), (1, 0, 4)),\n",
       " ((1, 0, 4), (1, 1, 7)),\n",
       " ((1, 1, 7), (1, 0, 4)),\n",
       " ((1, 1, 7), (1, 1, 7))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted(ex_rdd.cartesian(ex_rdd).collect())\n",
    "sorted(ex_rdd.cartesian(ex_rdd).filter(lambda x: x[0][0] == x[1][0]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((1,2), 5), ((2,4),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_res = ex_rdd\\\n",
    "    .cartesian(ex_rdd).filter(lambda x: x[0][0] == x[1][0])\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 58), ((0, 0), 17), ((1, 0), 31), ((0, 1), 31)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0.0),\n",
       " (0.0, 1.0, 0.8109999999999999),\n",
       " (0.0, 2.0, 0.0),\n",
       " (0.0, 3.0, 0.0)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_res.map(lambda xv: (xv[0][0], xv[0][1], xv[1])).saveAsTextFile('./ex_res.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 58), ((0, 0), 17)]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_res.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = ex_res.map(lambda xv: (xv[0][0], xv[0][1], xv[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 1 58'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([str(x) for x in ress[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./exres.txt', 'w') as f:\n",
    "    for t in ress:\n",
    "        f.write(' '.join([str(x) for x in t])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_res = sc.textFile(\"./ex_res.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1, 1, 58)', '(0, 0, 17)', '(1, 0, 31)', '(0, 1, 31)']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_res.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_rdd = rows_rdd.cartesian(rows_rdd)\\\n",
    "    .filter(lambda x: x [0][0] == x[1][0])\\\n",
    "    .map(lambda x: ((x[0][1],x[1][1]), x[0][2]*x[1][2]))\\\n",
    "    .reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 2121, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-2db5539fd1a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1305\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 2121, LAPTOP-C83P3CPF, executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:832)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:420)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:440)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:826)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1052)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:123)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:283)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:295)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:148)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\r\n"
     ]
    }
   ],
   "source": [
    "results = res_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_rdd = rows.map(lambda l: map(float, l.strip().split(' ')))\n",
    "rows_rdd = rows.map(lambda l: l.flatMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<map at 0x1d4c8c512e8>, <map at 0x1d4c8c51358>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows_rdd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
